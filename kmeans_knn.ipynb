{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/diogoflim/ProjIntegrador_PO_IA/blob/main/kmeans_knn.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicações de Pesquisa Operacional e Inteligência Artificial\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta aula, veremos dois procedimentos baseados em distâncias. O primeiro, k-médias, é um algoritmo de **clustering**, classificado como um procedimentos de aprendizado **não supervisionado**. Já o segundo procedimento, o k-NN, consiste em um algoritmo de aprendizado **supervisionado** que efetua tarefas de **classificação**. \n",
    "\n",
    "- K-médias - https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "- K-NN - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de dados\n",
    "\n",
    "\n",
    "Trabalharemos com um conjunto de dados do Legathum Prosperity Index que avalia países com base em 12 indicadores socioeconômicos. Os dados estão armazenados em uma planilha Excel. Para realizar a leitura, utilizaremos a biblioteca Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos dados\n",
    "url = 'https://raw.githubusercontent.com/diogoflim/ProjIntegrador_PO_IA/main/Dados/LegathumProsperityIndex.csv'\n",
    "\n",
    "df = pd.read_csv(url, index_col=0, sep = ';')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos inicialmente explorar o conjunto de dados com algumas estatísticas. Utilizaremos inicialmente o método describe. Em seguida, Vamos plotar alguns histogramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos utilizar a biblioteca referência em python para plotar gráficos\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (6 , 8))\n",
    "n = 0 \n",
    "for atributo in ['Economic Quality',\t'Education',\t'Enterprise Conditions']:\n",
    "    n += 1\n",
    "    plt.subplot(3 , 1 , n)\n",
    "    plt.subplots_adjust(hspace = 1 , wspace = 1)\n",
    "    plt.hist(df[atributo])\n",
    "    plt.title('Histograma do atributo {}'.format(atributo))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercício: Repita o procedimento acima escolhendo outros atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prodecimentos de Normalização\n",
    "\n",
    "Em algoritmos baseados em distância, assim como em outros modelos de Aprendizado de Máquina, é comum efetuar algumas etapas de pré-processamento de dados. Uma dessas etapas é a normalização da matriz de atributos.\n",
    "\n",
    "\n",
    "A normalização de dados refere-se ao processo de escalar os atributos de um conjunto de dados de forma que eles estejam dentro de um intervalo específico, como [0, 1] ou [-1, 1]. \n",
    "\n",
    "Isso pode ser feito de várias maneiras, e o scikit-learn fornece duas abordagens comuns: \n",
    "- Min-Max Scaling\n",
    "- Z-score Standardization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max Scaling, também conhecido como normalização intervalar [0, 1], é uma técnica que redimensiona os valores de um atributo para que fiquem dentro do intervalo especificado. \n",
    "\n",
    "A fórmula para Min-Max Scaling é dada por:\n",
    "\n",
    "$$X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- X: Valor original do atributo\n",
    "- $X_{\\text{min}}$: Valor mínimo do atributo no conjunto de dados\n",
    "- $X_{\\text{max}}$: Valor máximo do atributo no conjunto de dados\n",
    "\n",
    "\n",
    "Por sua vez, a Z-score Standardization, também conhecida como padronização, transforma os valores de uma característica para que tenham uma média de 0 e um desvio padrão de 1. \n",
    "\n",
    "A fórmula para Z-score Standardization é dada por:\n",
    "\n",
    "$$X_{\\text{scaled}} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Onde:\n",
    "- X: Valor original do atributo\n",
    "- $\\mu$: Média do atributo no conjunto de dados\n",
    "- $\\sigma$:Desvio padrão do atributo no conjunto de dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os métodos\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando os procedimentos\n",
    "min_max_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Realizando as transformações\n",
    "df_minmax = min_max_scaler.fit_transform(df)\n",
    "df_standardized = standard_scaler.fit_transform(df)\n",
    "\n",
    "# Passando os dados normalizados para DataFrames\n",
    "df_minmax = pd.DataFrame(df_minmax, columns=df.columns, index = df.index)\n",
    "df_standardized = pd.DataFrame(df_standardized, columns=df.columns, index = df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando o k-médias\n",
    "\n",
    "Utilizaremos os dados normalizados com o procedimento intervalar. Para a aplicação do k-médias, basta utilizarmos o procedimento da biblioteca sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 3 # Faremos o teste com 3 clusters\n",
    "\n",
    "# Instanciando o modelo K-Means\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "# Ajustando o modelo aos dados normalizados\n",
    "kmeans.fit(df_minmax)\n",
    "\n",
    "# Adicionando os rótulos de cluster aos dados originais\n",
    "df['Cluster Labels'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É comum no uso do k-means que testemos qual a quantidade ideal de clusters. \n",
    "\n",
    "Para isso, procedimentos comuns são:\n",
    "- Curva do cotovelo;\n",
    "- Curva da silhueta;\n",
    "\n",
    "Vejamos o funcionamento da curva do cotovelo, onde testaremos valores de $k$ de 1 a 10 e calcularemos a inércia para cada um. \n",
    "\n",
    "Em seguida, a curva do cotovelo será plotada para visualizarmos a tendência. \n",
    "\n",
    "O valor de k ideal será aquele em que a curva começa a se achatar, formando um \"cotovelo\". Assim, escolhemos um valor de k que corresponda ao ponto onde a inércia começa a diminuir a uma taxa menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for k in range(1, 11):  # Teste valores de k de 1 a 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df_minmax)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plote a curva do cotovelo\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 11), inertia, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Número de Clusters (k)')\n",
    "plt.ylabel('Inércia')\n",
    "plt.title('Curva do Cotovelo para Escolha de k')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando o k-NN\n",
    "\n",
    "Como o k-NN consiste em um procedimento de Aprendizado Supervisionado, trabalhamos com a presença de rótulos para os exemplos de treinamento.\n",
    "\n",
    "Para efeitos ilustrativos, vamos aproveitar nesse momento os rótulos atribuídos na clusterização do k-médias. Lembre que adicionamos uma coluna com o rótulo o nosso DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo as três últimas colunas\n",
    "df.iloc[:, -3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, vamos separar o DataFrame em uma matriz de atributos \"X\" e um vetor de rótulos \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(columns = [\"Cluster Labels\"]))\n",
    "y = np.array(df[\"Cluster Labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de Atributos\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetor de rótulos\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando a matriz de atributos.\n",
    "\n",
    "X = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostragem\n",
    "\n",
    "\n",
    "Ao aplicar um modelo de aprendizado de máquina, é importante que utilizemos alguma técnica de amostragem. Por exemplo, no tradicional procedimento holdout, separamos os dados em dois conjuntos:\n",
    "\n",
    "- Conjunto de treinamento: onde treinaremos o algoritmo de aprendizado;\n",
    "- Conjunto de teste: onde testaremos o modelo aprendido e verificaremos a performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que os dados foram separados, vamos treinar nosso algoritmo nos dados de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vizinhos = 3 # Número de vizinhos que utilizaremos inicialmente\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors = k_vizinhos) #Instanciando o modelo\n",
    "knn_classifier.fit(X_train, y_train) # Treinando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos aplicar o algoritmo treinado nos dados de teste para realizar previsões das classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos medir a acurácia atingida pelo nosso modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acuracia = accuracy_score(y_test, y_pred)\n",
    "print(f\"A acurácia obtida no conjunto de teste foi: {acuracia:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso modelo previu corretamente todo o conjunto treinado, atingindo uma acurácia de 100%.\n",
    "\n",
    "Perceba que isso não necessariamente ocorre. Pode ter sido ao acaso. Vamos refazer nosso experimento com uma outra semente aleatória na separação dos conjunto de treinamento e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "\n",
    "k_vizinhos = 3\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors = k_vizinhos)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "acuracia = accuracy_score(y_test, y_pred)\n",
    "print(f\"A acurácia obtida no conjunto de teste foi: {acuracia:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agora nossa acurácia foi de apenas 94,12%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação Cruzada\n",
    "\n",
    "Para minimizar os efeitos problemáticos da amostragem Holdout, podemos partir para uma outra técnica de amostragem. Vejamos a técnica cross validation.\n",
    "\n",
    "\n",
    "1. Divisão dos dados: Inicialmente, o conjunto de dados é dividido em duas partes: um conjunto de treinamento e um conjunto de teste. O conjunto de treinamento é usado para treinar o modelo, enquanto o conjunto de teste é reservado para avaliar o desempenho do modelo.\n",
    "\n",
    "2. Iteração: No procedimento de validação cruzada, essa divisão em treinamento e teste é repetida várias vezes, chamadas \"fold\" ou dobras. Cada vez, uma parte diferente do conjunto de dados é usada como conjunto de teste, enquanto o restante é usado como conjunto de treinamento. Por exemplo, em uma validação cruzada de 5-fold, o conjunto de dados é dividido em 5 partes iguais, e o processo é repetido 5 vezes.\n",
    "\n",
    "3. Treinamento e Avaliação: Durante cada iteração, um modelo é treinado no conjunto de treinamento e avaliado no conjunto de teste. As métricas de desempenho, como a precisão, podem ser registradas para cada iteração.\n",
    "\n",
    "4. Cálculo das Métricas Finais: Após todas as iterações, as métricas de desempenho (por exemplo, precisão média) são calculadas a partir das métricas registradas em cada dobra. Isso fornece uma estimativa mais confiável do desempenho do modelo em dados não vistos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validação cruzada 5-fold\n",
    "num_folds = 5\n",
    "acuracias_obtidas = cross_val_score(knn_classifier, X, y, cv=num_folds)\n",
    "\n",
    "\n",
    "for i, score in enumerate(acuracias_obtidas):\n",
    "    print(f\"Fold {i + 1}: Acurácia = {score:.4f}\")\n",
    "\n",
    "# Calculando a acurácia média\n",
    "acuracia_media = np.mean(acuracias_obtidas)\n",
    "print(f\"Acurácia Média Obtida no Experimento: {acuracia_media:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De maneira geral, o nosso procedimento obteve bons resuldados, com uma acurácia média de 97,02%.\n",
    "\n",
    "É importante lembrar que nesse teste utilizamos rótulos vindos do procedimento k-means. Isso não é usual. Normalmente, os rótulos são considerados respostas, por exemplo, atribuídas por um especialista. \n",
    "\n",
    "Um exemplo seria o diagnóstico (rótulos) históricos atribuídos por médicos de um hospotal a pacientes que chegam com determinados sintomes (matriz de atributos). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
